# SPDX-License-Identifier: MIT
# Copyright (c) 2025-present, symseq Contributors

"""
artificial_grammar.py

This module contains the ArtificialGrammar class that can be used to generate artificial grammars akin to those
used in psycholinguistic experiments.
"""

import copy
import logging
from multiprocessing import cpu_count
import numpy as np
import pandas as pd
import warnings
import networkx as nx
import random
from concurrent.futures import ProcessPoolExecutor, as_completed

# internal imports
from symseq.core.sequencer import SymbolicSequencer
from symseq.core.state import State
from symseq.grammars.ag import agl_dataset
from symseq.grammars.ag import utils, presets
from symseq.grammars.ag.generator import generate_random_grammar, grammar_with_complexity
from symseq.utils.io import get_logger, save_pickle
from symseq.utils.strtools import string_as_symbols
from symseq.viz.ag_viz import plot_grammar


logger = get_logger(__name__)


class ArtificialGrammar(SymbolicSequencer):
    """
    Symbolic sequence with transition rules specified by a directed graph, along with corresponding transition
    probabilities. The sequences are generated by traversing the graph. This formalism can be used to define any
    sequence, provided the states and transitions can be specified (each symbol as a state)

    See also, e.g.:
    ----------
    [1] - Pothos, E. M. (2010). An entropy model for artificial grammar learning. Frontiers in Psychology, 1(June), 16.
    [2] - Duarte, R., Seriès, P., & Morrison, A. (2014). Self-Organized Artificial Grammar Learning in Spiking Neural
    Networks. In Proceedings of the 36th Annual Conference of the Cognitive Science Society (pp. 427–432).
    [3] - Bollt, E. M., & Jones, M. a. (2000). The Complexity of Artificial Grammars. Nonlinear Dynamics Psychology and
    Life Sciences, 4(2), 153–168.
    [4] - Pothos, E. M. (2007). Theories of artificial grammar learning. Psychological Bulletin, 133(2),
    227–244.
    """

    def __init__(
        self,
        label: str,
        states: list[str],
        alphabet: list[str],
        transitions: list[tuple],  # TODO add option to provide transitions as a matrix, which must then be parsed
        start_states: list[str],
        terminal_states: list[str],
        eos: str = "#",
        rng: np.random.Generator | None = None,
        seed: int = 42,
        validate: bool = True,
        build_graph: bool = True,
        metadata: dict | None = None,
        verbose: bool = True,
    ):
        """

        Parameters
        ----------
        states: list
            A list containing the states of the grammar, i.e., the nodes of the directed graph. Expected format is
            "SymbolName" or "SymbolName(index)", where SymbolName is the name of the symbol and index is the index of
            the state in the FSM. For example, "A(1)", "B (2)" or "C" are all valid state names.

            #For simplicity and robustness of implementation, the nodes should correspond to the individual
            #(unique) symbols that constitute the language
        alphabet: list
            A list containing the unique symbols that ought to be represented.
            In many cases, these symbols are different from the states, given that the same symbol may correspond to
            several different nodes, in which case, the different states for the same symbol are numbered (see examples)
        transitions: list
            List of tuples with the structure (source_state, target_state, transition probability).
            e.g. [('a','b',0.1), ('a','c',0.3)]
         start_states:
             A list containing the possible start states. See `states` for the format.
         terminal_states:
             A list containing the possible terminal states. See `states` for the format.
         eos: str, optional
             Symbol to use as eos marker. Defaults to '#'.
         rng: NumPy RandomState object
             Random number generator state object (optional). Either None or a numpy.random.default_rng object.
             If None, a new default generator is created with seed `seed`.
         seed : int
             Random seed for reproducibility, default is 42. This parameter is ignored if `rng` is not None.
         validate: bool
             Whether to validate the grammar. Defaults to True.
         build_graph: bool
             Whether to build the graph. Defaults to True.
         verbose: bool
             Whether to print the grammar. Defaults to True.

        Raises
        ------
        ValueError
            If no start states are provided.
        ValueError
            If no terminal states are provided.
        """
        self.rng = rng if rng is not None else np.random.default_rng(seed)
        self.metadata = metadata

        # consistency checks
        if len(start_states) == 0:
            raise ValueError("No start states provided.")

        if len(terminal_states) == 0:
            raise ValueError("No terminal states provided.")

        # initialize the base class
        super().__init__(
            label=label,
            alphabet_size=len(alphabet),
            alphabet=alphabet,
            rng=np.random.default_rng(self.rng.integers(1, 1000)),
            verbose=verbose,
        )
        self.eos = eos  # end of sequence symbol

        # parse the provided states into proper format and State objects
        self.states, self.start_states, self.terminal_states = self._parse_states(states, start_states, terminal_states)

        # list (str, not States) and table of all transitions, including the EOS sink
        self.transitions, self.transition_table = self._parse_transitions(transitions)

        if validate:
            self.validate()

        if build_graph:
            self.graph = self.create_graph()

        if verbose:
            ArtificialGrammar.print(self)  # call print method in this class

    @classmethod
    def from_preset(cls, preset_name, seed=42) -> "ArtificialGrammar":
        """
        Create an ArtificialGrammar from a preset.

        Parameters
        ----------
        preset_name : str
            Name of the preset to use.

        Returns
        -------
        ArtificialGrammar
            The ArtificialGrammar object.
        """
        if preset_name not in presets.__dict__:
            raise ValueError(f"Preset {preset_name} not found.")

        preset = presets.__dict__[preset_name]
        return cls(seed=seed, **preset)

    @classmethod
    def from_constraints(
        cls,
        label: str = "Random artificial grammar",
        alphabet_size: int = 4,
        ambiguities: int = 0,
        ambiguity_depth: int = 0,
        n_start_states: int = 1,
        n_terminal_states: int = 1,
        transition_density: float = 0.25,
        assume_equiprobable: bool = True,
        min_string_length: int = 1,
        max_iter: int = int(1e5),
        eos: str = "#",
        rng: np.random.Generator | None = None,
        seed: int = 42,
        verbose: bool = True,
        target_complexity: None | float = None,
        **synthesis_kwargs,
    ) -> "ArtificialGrammar":
        """
        Generate an artificial grammar following the specified constraints and properties.
        Formally, each grammatical (valid) string will end with the EOS symbol.

        Parameters
        ----------
        label : str
            Grammar label.
        alphabet_size : int
            Size of the alphabet or number of symbols.
        ambiguities : int
            How many symbols can be repeated.
        ambiguity_depth : int
            Specifies the number of different instances of each repeatable symbol. Only relevant if ambiguities > 0.
        start_states : int
            Number of initial states.
        terminal_states : int
            Number of terminal states corresponding to regular symbols (not EOS), with direct transitions to the
            terminal state (EOS).
        transition_density : float
            Density of the transition matrix.
        assume_equiprobable : bool
            Assume equiprobable outgoing transitions from each state.
        min_string_length : int
            Minimum length of valid strings in the grammar.
        max_tries : int
            Maximum number of attempts to generate a valid grammar.
        rng : np.random.RandomState or None
            Random number generator instance for reproducibility. If None, a new default generator is created with
            seed `seed`.
        seed : int
            Random seed for reproducibility, default is 42. This parameter is ignored if `rng` is not None.
        verbose : bool
            Display progress information.

        Returns
        -------
        ArtificialGrammar
            The ArtificialGrammar object.

        Notes
        -----
        """
        if target_complexity is None:
            grammar_pars = generate_random_grammar(
                label=label,
                alphabet_size=alphabet_size,
                ambiguities=ambiguities,
                ambiguity_depth=ambiguity_depth,
                n_start_states=n_start_states,
                n_terminal_states=n_terminal_states,
                transition_density=transition_density,
                assume_equiprobable=assume_equiprobable,
                min_string_length=min_string_length,
                max_iter=max_iter,
                eos=eos,
                rng=rng,
                seed=seed,
                verbose=verbose,
            )
            return cls(rng=rng, seed=seed, verbose=verbose, **grammar_pars)
        else:
            grammar_pars, metadata = grammar_with_complexity(
                label=label,
                alphabet_size=alphabet_size,
                ambiguities=ambiguities,
                ambiguity_depth=ambiguity_depth,
                n_start_states=n_start_states,
                n_terminal_states=n_terminal_states,
                transition_density=transition_density,
                assume_equiprobable=assume_equiprobable,
                min_string_length=min_string_length,
                max_iter=max_iter,
                eos=eos,
                rng=rng,
                seed=seed,
                verbose=verbose,
                target_complexity=target_complexity,
                **synthesis_kwargs,
            )
            return cls(rng=rng, seed=seed, verbose=verbose, metadata=metadata, **grammar_pars)

    ####################################

    def create_graph(self) -> nx.DiGraph:
        """
        Create a networkX graph object from the grammar.

        Returns
        -------
        nx.DiGraph
            The graph object.
        """
        graph = nx.DiGraph()

        for state in self.states:
            label = str(state)  # symbol + index
            graph.add_node(label, state=state)  # save the state object in the node

        for transition in self.transitions:
            source, target, probability = transition
            graph.add_edge(source, target, weight=probability)

        return graph

    def _parse_transitions(
        self,
        transitions,
    ):
        """
        Parse the provided list of transitions into a list of transition tuples and create the transition table
        (look-up table with all allowed transitions and their probabilities).


        Parameters
        ----------
        transitions : list
            A list of tuples containing the transitions of the grammar. Each tuple should contain the following elements:
            (source_state, destination_state, probability). The source_state and destination_state should be strings
            representing the names of the source and destination states, respectively. The probability should be a
            float representing the probability of transitioning from the source state to the destination state.

        Returns
        -------
        transitions : list
            A list of tuples containing all transitions of the grammar.
        table : np.ndarray
            A NumPy array representing the transition table.

        Notes
        -----
        - The transition table is normalized such that the outgoing transition probabilities sum to 1.
        - If the grammar contains terminal states, the transition table is augmented with transitions to the EOS
          marker. The outgoing transition probabilities of these states are normalized to 1.

        """
        assert transitions and len(transitions) > 0, "No transitions provided"

        table = np.zeros((len(self.states), len(self.states)))

        terminal_states_noeos = set()  # list of terminal states without transitions to EOS
        # add transitions from terminal states to the EOS if not present, for now all with probability 1
        for state in self.terminal_states:
            trans_to_eos = [t for t in transitions if t[0] == state and t[1] == self.eos]
            if len(trans_to_eos) == 0:
                transitions.append((state.tostring(), self.eos, 1.0))  # probability will be normalized later
                terminal_states_noeos.add(state.tostring())

        # populate transition table from list
        state_idxs = {s: i for i, s in enumerate(self.states)}
        for src_state, tgt_state, prob in transitions:
            src_idx = state_idxs[src_state]
            tgt_idx = state_idxs[tgt_state]
            table[tgt_idx, src_idx] = prob

        # eos_idx = [i for i, s in enumerate(self.states) if s == self.eos]  # index of the EOS, we assume it's present
        # normalize the outgoing transitions of terminal states which did not previously have a transition to EOS
        for state in terminal_states_noeos:
            state_idx = [i for i, s in enumerate(self.states) if s == state][0]  # index of the state
            table[:, state_idx] /= table[:, state_idx].sum()  # normalize the outgoing transitions
            logger.info(
                f"Added transition to EOS ({self.eos}) from terminal state {state}"
                f" and normalized all outgoing probabilities."
            )

        return transitions, table

    def _parse_states(self, states, start_states, terminal_states):
        """
        Parse the states, start_states and terminal_states arguments into a list of State objects.

        Parameters
        ----------
        states : list
            A list containing the states of the grammar, i.e., the nodes of the directed graph. Expected format is
            "SymbolName" or "SymbolName(index)", where SymbolName is the name of the symbol and index is the index of
            the state in the FSM. For example, "A(1)", "B (2)" or "C" are all valid state names.

            #For simplicity and robustness of implementation, the nodes should correspond to the individual
            #(unique) symbols that constitute the language
        start_states : list
            A list containing the possible start symbols
        terminal_states : list
            A list containing the possible terminal symbols

        Returns
        -------
        states_list : list
            A list of State objects, sorted by symbol name.
        start_states_list : list
            A list of State objects, sorted by symbol name.
        terminal_states_list : list
            A list of State objects, sorted by symbol name.

        Raises
        ------
        ValueError
            If the symbol name of a state is not in the alphabet.
        """
        states_list = []
        start_states_list = []
        terminal_states_list = []

        for s in states:
            state = State.from_string(s)  # create a State object

            # check if the symbol name is in the alphabet
            if state.symbol not in self.alphabet and state.symbol != self.eos:
                raise ValueError(f"State {state} with symbol {state.symbol} is not in the alphabet.")

            # add to the list of all, start and terminal states as appropriate
            states_list.append(state)

            if s in start_states:
                start_states_list.append(state)
                state.start = True
            if s in terminal_states:
                terminal_states_list.append(state)
                state.terminal = True

        # add the eos state if it's not already there
        if self.eos not in states_list:
            states_list.append(State.from_string(self.eos))

        # indexed states, unique state in the FSM  - TODO make this clear in the docs
        # sort the lists by symbol name to ensure consistency
        states_list = sorted(states_list, key=lambda s_: s_.symbol)
        start_states_list = sorted(start_states_list, key=lambda s_: s_.symbol)
        terminal_states_list = sorted(terminal_states_list, key=lambda s_: s_.symbol)

        return states_list, start_states_list, terminal_states_list

    def print(self):
        """
        Displays all the relevant information.
        """
        logger.info("***************************************************************************")
        if self.label is not None:
            logger.info(("Generative mechanism: %s" % self.label))
        logger.info("Alphabet: {0}".format(self.alphabet))
        logger.info("Unique states: {0}".format(self.states))
        logger.info("Start states: {0}".format(self.start_states))
        logger.info("Terminal states: {0}".format(self.terminal_states))
        self.get_transition_table(verbose=True)

    def get_transition_table(self, full=True, correct_terminal_sink=False, verbose=True, binary=False):
        """
        Return a copy of the look-up table with all allowed transitions and their probabilities. If necessary,
        correct for terminal (sink) -> start transitions.

        Parameters
        ----------
        full: bool
            If True, returns the full transition table, including all states and EOS sink. Otherwise, it returns only
            the terminal states without the EOS. Note that in this case the transition probabilities are not corrected
            and the outgoing probabilities for some states may not sum to 1.
        correct_terminal_sink: bool
            If True, adds a transition from the EOS to all initial states with probability 1/n_initial_states.
        verbose: bool
            Display table. Defaults to True.

        Returns
        -------
        Returns a NumPy array (np.ndarray) as the transition table, where the rows (columns) are targets (sources).

        """
        eos_idx = [i for i, s in enumerate(self.states) if s == self.eos]  # index of the EOS, we assume it's present
        table = copy.deepcopy(self.transition_table)

        # add transitions from the terminal states to the initial states with probability 1/n_initial_states
        if correct_terminal_sink:
            for state in self.start_states:
                state_idx = [i for i, s in enumerate(self.states) if s == state][0]  # index of the state
                table[state_idx, eos_idx] = 1.0 / len(self.start_states)

        states_as_str = [s.tostring() for s in self.states]
        if not full:
            # remove the EOS from the table (corresponding row and column)
            table = table[np.ix_(np.arange(table.shape[0]) != eos_idx, np.arange(table.shape[1]) != eos_idx)]
            logger.warning(
                f"Removed transitions to EOS ({self.eos}) from terminal states. Outgoing probabilities"
                f" from terminal states may not sum to 1 any more."
            )
            states_as_str.remove(self.eos)

        if verbose:
            msg = "Transition table (target x source)"
            if correct_terminal_sink:
                msg += " with corrected terminal probabilities"
            if not full:
                msg += " (EOS sink removed)"

            # select correct set of states and create the table DataFrame for display
            df = pd.DataFrame(table, columns=list(states_as_str), index=list(states_as_str))
            print(df)

            logger.info(msg + ":")
            logger.info("\n" + str(df))

        if binary:
            table = (table > 0.0).astype(int)
        return table

    def get_binary_transition_table(self, full=True, correct_terminal_sink=True, verbose=False) -> np.ndarray:
        """
        Return a copy of the look-up table with all allowed transitions as an adjacency matrix. See also
        `get_transition_table`.

        Parameters
        ----------
        full: bool
            If True, returns the full transition table, including all states and EOS sink. Otherwise, it returns only
            the terminal states without the EOS. Note that in this case the transition probabilities are not corrected
            and the outgoing probabilities for some states may not sum to 1 any more. Defaults to True.
        correct_terminal_sink: bool
            If True, adds a transition from the EOS to all initial states with probability 1/n_initial_states. Defaults
            to True.
        verbose: bool
            Display table. Defaults to False.

        Returns
        -------
        Returns a NumPy array (np.ndarray) as the transition table, where the rows (columns) are targets (sources).

        """
        transitions = self.get_transition_table(full=full, correct_terminal_sink=correct_terminal_sink, verbose=verbose)
        transitions = transitions > 0.0
        binary_transitions = transitions.astype(int)
        return binary_transitions

    def validate(self):
        """
        Verify that all the start and end states are members of the state set and if the alphabet is
        different from the states.

        Returns
        -------
        bool
            True if grammar is valid, False otherwise.

        Raises
        ------
        AssertionError
            If start_states or end_states are not within the states.
        """
        assert set(self.start_states).issubset(set(self.states)), "Start states not in states"
        assert set(self.terminal_states).issubset(set(self.states)), "Terminal states not in states"
        assert len([s for s in self.terminal_states if s == self.eos]) == 0, "Terminal states should not contain EOS"

        if not set(self.alphabet).issubset(set(self.states)):
            test_var = set(self.alphabet).difference(set(self.states))
            logger.debug(test_var)
            return test_var
        else:
            return True

    def _generate_path(self, source, target, max_length=1000):
        """
        Generate a valid stochastic path from source to target using a transition matrix.

        Parameters:
        -----------
            P : np.ndarray
              Transition probability matrix of shape (n_states, n_states).
            source (int): Index of the start state.
            target (int): Index of the terminal state.
            max_len (int): Maximum number of transitions to avoid infinite loops.

        Returns:
        list[int]: List of state indices forming the path, including source and target.
                Returns None if no path is found within max_len.
        """
        path = [source]
        current = source

        for _ in range(max_length):
            if current == target:
                return path

            probs = self.transition_table[:, current]
            if probs.sum() == 0:
                return None  # Dead end

            next_state = self.rng.choice(len(probs), p=probs)
            path.append(next_state)
            current = next_state

        return None  # No path found within limit

    def generate_string(
        self,
        min_length: int = 0,
        max_length: int = int(1e4),
        length_range: tuple | None = None,
        remove_eos: bool = True,
        as_states: bool = False,
        max_iter: int = int(1e3),
    ) -> list[str]:
        """
        Generate a (grammatical) single string by traversing the grammar. String length is controlled by the arguments
        `min_length`, `max_length`, and `length_range`.

        Parameters
        ----------
        min_length : int
            Minimum string length, including all symbols except the EOS. Defaults to 0.
        max_length : int
            Maximum string length, including all symbols except the EOS. Defaults to 1e4.
        length_range : tuple
            Tuple of length (min, max) for the length of each string. If None, there will be no constraints on the
            string length. If any of the two values is None, the corresponding constraint will be ignored. Defaults
            to None.
        remove_eos : bool
            If True, remove eos (e.g., '#') markers from the strings. Defaults to True.
        as_states : bool
            If True, the string will be returned as a list of states (e.g., A, B3) instead of symbols (e.g., A, B).
            Defaults to False.
        max_iter : int (default: int(1e4))
            Maximum attempts (grammar traversals) to generate a valid string.

        Returns
        -------
        string : list of str
            String as a list of symbols.

        Raises
        ------
        AssertionError
            If no allowed transitions are found from the current node.
        RuntimeError
            If a grammatical string of maximum length 'max_length' cannot be generated after 'max_iter' attempts.
        """
        valid = False
        tmp_max_tries = max_iter
        string = []

        if length_range is not None:
            if not isinstance(length_range[0], (int, float)) or not isinstance(length_range[1], (int, float)):
                raise ValueError("Length range must be a tuple of two integers.")
            min_length, max_length = length_range

        eos_idx = [i for i, s in enumerate(self.states) if s == self.eos][0]  # index of the EOS, we assume it's present
        start_state_idxs = [i for i, s in enumerate(self.states) if s in self.start_states]
        # attempt generating a string until a valid one is found
        while not valid and max_iter > 0:
            start_state_idx = self.rng.choice(start_state_idxs)
            path = self._generate_path(start_state_idx, eos_idx, max_length=max_length + 1)  # + 1 for EOS

            if path:
                string = [self.states[i].tostring() for i in path]  # convert table indices to states
                # check string validity by also considering the length constraints
                valid = min_length <= len(string) - 1 <= max_length
                valid = valid and string[-1] == self.eos  # valid if the last symbol is the EOS

            max_iter -= 1

        if max_iter == 0:
            raise RuntimeError(
                f"Could not generate a grammatical string that "
                f"satisfies the constraints after {tmp_max_tries} tries."
            )

        # convert to symbols if necessary by removing indices
        if not as_states:
            string = string_as_symbols(string)

        if remove_eos:
            string.remove(self.eos)

        return string

    def _is_symbol_format(self, string: list[str]) -> bool:
        """
        Check if the string contains only symbols (each item is in the alphabet) or also states (indexed).


        Parameters
        ----------
        string : list of str
            The string to check.

        Returns
        -------
        bool
            True if the string is in the symbol format, False otherwise.
        """
        return np.all([s in self.alphabet for s in string])

    def add_deviant(
        self,
        string: list[str],
        n_deviants: int = 1,
        max_iter: int = int(1e4),
        remove_eos: bool = True,
        verbose=False,
    ):
        """
        Make a string non-grammatical by introducing one or more deviants (errors) at random positions.

        Arguments
        ---------
        string : list of str
            The string to make non-grammatical. It can be defined as a list of states (possibly indexed) or a list of
            symbols.
        n_deviants : int
            Number of deviants to introduce.
        max_iter : int
            Maximum number of tries to find a valid deviant. Defaults to int(1e4).
        remove_eos : bool
            Whether to remove the eos marker from the string. Defaults to True.

        Returns
        -------
        nongramm_string : list of str
            The non-grammatical string specified in the same format (states or symbols) as the input string.

        Raises
        ------
        ValueError
            If the number of deviants is not greater than 1, if the string length is less than 2, or if the
            number of deviants exceeds the string length.
        """
        # sanity checks
        if len(string) < 2:
            raise ValueError("String must have at least 2 symbols to check transitions.")

        if n_deviants < 1:
            raise ValueError("Number of deviants must be greater than 0.")

        found_ng = False  # flag for finding non-grammatical string
        ref_string = copy.deepcopy(string)  # reference copy of the deviant string at the symbol level

        inp_as_symbols = self._is_symbol_format(string)
        if inp_as_symbols:
            deviant_elem_set = set(self.alphabet)  # symbol level, no indexed states
        else:
            deviant_elem_set = [s.tostring() for s in self.states if s != self.eos]  # state level with indices

        # for consistency, add eos to the end of the deviant string. May be removed at the end
        if ref_string[-1] != self.eos:
            ref_string.append(self.eos)

        length = len(ref_string)
        candidate_pos = list(range(length - 1))

        if not candidate_pos:
            raise RuntimeError("No available positions to introduce deviants.")
        if n_deviants > len(candidate_pos):
            raise ValueError("Number of deviants exceeds the number of available positions.")

        tmp_max_tries = max_iter
        nongramm_string = []
        while max_iter > 0 and not found_ng:
            found_ng = True
            nongramm_string = copy.deepcopy(ref_string)  # always start with the original/reference string
            # Randomly choose positions for deviants
            deviant_pos = self.rng.choice(candidate_pos, size=n_deviants, replace=False)

            for pos in deviant_pos:
                curr_symbol = State.from_string(ref_string[pos]).symbol
                # list of possible deviant symbols chosen deterministically
                possible_deviants = sorted(
                    [elem for elem in deviant_elem_set if State.from_string(elem).symbol != curr_symbol]
                )
                deviant = str(self.rng.choice(possible_deviants))
                nongramm_string[pos] = deviant

                # break loop if the new string is grammatical
                if self.is_grammatical(nongramm_string):
                    found_ng = False
                    # At least one position could not be filled, so restart the loop
                    max_iter -= 1
                    break

        if not found_ng:
            raise RuntimeError(f"Could not make string {string} non-grammatical in {tmp_max_tries} tries!")

        if remove_eos:
            nongramm_string.remove(self.eos)

        return nongramm_string

    def is_grammatical(self, string: list[str]) -> bool:
        """
        Checks if a given string is grammatical based on the grammar's transitions. Although the string can be
        represented as a list of states or symbols, grammaticality is always evaluated based on the non-indexed
        states (symbols): if ['A', 'B', 'C', '#'] is grammatical, then states with any indices, such as
        ['A', 'B', 'C(1)', '#'] or ['A(3)', 'B(0)', 'C(15)', '#'], will also be considered grammatical.

        Parameters
        ----------
        string : list of str
            The string to check, represented as a list of states or symbols.

        Returns
        -------
        bool
            True if the string is grammatical, False otherwise.
        """
        string_symbols = string_as_symbols(string)  # store string as symbols and remove indices

        # Generate a set of valid transitions for fast lookup
        valid_transitions = set(
            [(State.from_string(src).symbol, State.from_string(tgt).symbol) for src, tgt, _ in self.transitions]
        )

        # Check if the first state is a valid start state
        if string_symbols[0] not in string_as_symbols(self.start_states):
            return False

        # Iterate over the string and check for valid transitions
        for i in range(len(string) - 1):
            if (string_symbols[i], string_symbols[i + 1]) not in valid_transitions:
                return False  # A transition is invalid

        return True  # All transitions are valid

    def generate_nongrammatical_string(
        self,
        n_deviants: int = 1,
        min_length: int = 0,
        max_length: int = int(1e4),
        length_range: tuple | None = None,
        remove_eos: bool = True,
        as_states: bool = False,
        max_iter=int(1e4),
    ):
        """
        Convenience function to generate a non-grammatical string.

        For a given string set and a fraction of non-grammatical strings, introduce an appropriate number of deviants
        to create new, non-grammatical strings.

        Parameters
        ----------
        n_deviants : int
            Number of deviants to introduce. Defaults to 1.
        min_length : int
            Minimum string length not including the EOS. Defaults to 0.
        max_length : int
            Maximum string length not including the EOS. Defaults to 1e4.
        length_range : tuple of int | None
            Tuple of length (min, max) for the length of each string or None. If None, there will be no constraints on the
            string length. If tuple, both values must be specified. Defaults to None.
        remove_eos : bool
            If True, remove eos (e.g., '#') markers from the strings. Defaults to True.
        as_states : bool
            Whether to remove indices (e.g., A1 -> A) from the generated strings. Defaults to False.
        max_iter : int
            Maximum attempts to generate a (grammatical or non-grammatical) string.

        Returns
        -------
        nongramm_string : list of str
            The non-grammatical string.

        Raises
        ------
        RuntimeError
            If a grammatical string satisfying the constraints is not found after `max_iter` attempts.

        """
        string = self.generate_string(
            min_length=min_length,
            max_length=max_length,
            length_range=length_range,
            remove_eos=remove_eos,
            as_states=as_states,
            max_iter=max_iter,
        )

        nongramm_string = self.add_deviant(
            string,
            n_deviants=n_deviants,
            remove_eos=remove_eos,
        )

        if not as_states:
            nongramm_string = string_as_symbols(nongramm_string)

        return nongramm_string

    def generate_string_set(
        self,
        n_samples,
        length_range=(1, 1000),
        nongramm_fraction=0.0,
        n_deviants: int = 1,
        allow_repetitions: bool = True,
        remove_eos: bool = True,
        as_states: bool = False,
        max_iter=int(1e4),
        n_proc=1,
        oversample_factor=1.0,
    ) -> tuple[list[list[str]], list[bool]]:
        """
        Generates a total of `n_samples` strings, a fraction `nongramm_fraction` of which are non-grammatical.
        Each string is generated randomly according to the grammar. Non-grammatical strings are created by generating
        a grammatical string and then introducing one or more deviants (errors).

        Parameters
        ----------
        n_samples : int
            Total number of strings to generate.
        length_range : None or tuple
            String length, should be specified as interval [min_len, max_len]. If None is passed, there will be no
            constraints on the string length.
        nongramm_fraction : float
           Fraction of non-grammatical items to be introduced in the dataset.
        n_deviants : int
            Number of deviants to introduce if non-grammatical strings are generated. Defaults to 1.
        allow_repetitions: bool
            Allow repetitions of the same string(s) in the generated set. If False, the function may throw an error if
            it cannot generate a sufficient number of unique strings.
        remove_eos : bool
            If True, remove eos (e.g., '#') markers from the strings.
        as_states : bool
            Whether to remove indices (e.g., A1 -> A) from the generated strings.
        max_iter : int
            Maximum attempts to generate a (grammatical or non-grammatical) string.
        n_proc : int
            Number of processes to use for parallelization. Defaults to 1.
        oversample_factor : float
            Factor to oversample the generated strings, after which the correct number of samples is selected. A value
            > 1.0 can be useful if unique strings are desired (`allow_repetitions=False`) and many collisions are
            expected during random sampling. Defaults to 1.5.

        Returns
        -------
        string_set : list of list of str
            List of grammatical (and possibly non-grammatical) strings generated, randomly shuffled.
        grammaticality : list of bool
            List of booleans indicating whether each string is grammatical or not.

        Warnings
        --------
        - If string repetitions are not allowed, the function may throw a warning if it cannot generate a sufficient
        number of unique strings.

        """

        def _generate_subset(n_target, worker_type, extra_kwargs=None):
            """Helper to generate either grammatical or non-grammatical string subsets in a parallelized manner."""
            subset_strings = []
            subset_unique = set()
            attempts = 0

            while len(subset_strings) < n_target:
                total = int(n_target * oversample_factor)
                per_worker = int(np.ceil(total / n_proc))

                gen_kwargs = {
                    "min_length": length_range[0],
                    "max_length": length_range[1],
                    "length_range": length_range,
                    "remove_eos": remove_eos,
                    "as_states": as_states,
                    "max_iter": max_iter,
                }
                if extra_kwargs:
                    gen_kwargs.update(extra_kwargs)

                # Master seed sequence for reproducibility
                master_ss = np.random.SeedSequence(self.rng.integers(1, 1000))
                child_seqs = master_ss.spawn(n_proc)

                # Generate strings in parallel
                results = []
                with ProcessPoolExecutor(max_workers=n_proc) as pool:
                    futures = [
                        pool.submit(
                            _generate_string_worker,
                            (self, worker_type, gen_kwargs, per_worker, child_seqs[i]),
                        )
                        for i in range(n_proc)
                    ]
                    for f in as_completed(futures):
                        results.extend(f.result())

                # Handle repetitions / uniqueness
                if allow_repetitions:
                    subset_strings.extend(results)
                else:
                    for r in results:
                        r_tuple = tuple(r)  # convert list to hashable tuple
                        if r_tuple not in subset_unique:
                            subset_strings.append(r)
                            subset_unique.add(r_tuple)

                attempts += 1
                if attempts > max_iter:
                    break

            if len(subset_strings) < n_target:
                logger.warning(f"Only {len(subset_strings)}/{n_target} {worker_type} strings generated")
                warnings.warn(f"Could not generate all {worker_type} strings", UserWarning)
            # truncate to desired number of strings
            if len(subset_strings) > n_target:
                subset_strings = subset_strings[:n_target]

            return subset_strings

        # Determine counts
        n_gramm_strings = np.rint((1 - nongramm_fraction) * n_samples).astype(int)
        n_nongramm_strings = np.rint(nongramm_fraction * n_samples).astype(int)
        logger.info(
            f"Generating {n_samples} ({n_gramm_strings} grammatical, {n_nongramm_strings} non-grammatical) strings..."
        )

        # select correct number of cpus for parallelization
        if n_proc is None or n_proc > cpu_count():
            n_proc = cpu_count()

        # generate sets
        gramm_string_set = _generate_subset(n_gramm_strings, "generate_string")
        nongramm_string_set = _generate_subset(
            n_nongramm_strings,
            "generate_nongrammatical_string",
            extra_kwargs={"n_deviants": n_deviants},
        )

        # combine grammaticality flags
        string_set = gramm_string_set + nongramm_string_set
        grammaticality = [True] * len(gramm_string_set) + [False] * len(nongramm_string_set)

        # deterministic shuffle
        combined = list(zip(string_set, grammaticality))
        random.Random(42).shuffle(combined)
        string_set[:], grammaticality[:] = zip(*combined)

        return string_set, grammaticality

    def generate_balanced_agl(self, **kwargs):
        """Wrapper for agl_dataset.generate_balanced_agl"""
        return agl_dataset.generate_balanced_agl(self, **kwargs)

    def generate_feature_string_set(
        self,
        n_samples,
        feature_constraints: dict,
        train_set: list[list[str]],
        length_range=(1, 1000),
        # nongramm_fraction=0.0,
        # n_deviants: int = 1,
        # allow_repetitions: bool = True,
        # remove_eos: bool = True,
        # as_states: bool = False,
        # max_iter=int(1e5),
        n_proc=4,
        verbose=False,
    ) -> list[tuple[list[str], dict]]:
        """
        TODO optimized this and remove length constraint
        """
        assert length_range[1] <= 20, "Length range is too large, consider using a smaller range (<20) for now"
        n_proc = min(n_proc or cpu_count(), cpu_count())  # number of processes to use

        start_nodes = [str(s) for s in self.start_states]
        props = {
            "length_range": length_range,
            "eos": self.eos,
        }
        # generate all strings with constrained properties
        string_list = utils.all_paths_as_strings(self.graph, start_nodes, n_proc, **props)
        logging.info(f"Computed {len(string_list)} strings with valid property constraints.")

        str_feature_map = {}
        # compute features for each string, in parallel
        with ProcessPoolExecutor(max_workers=n_proc) as executor:
            futures = [executor.submit(utils.process_feature, s, train_set, feature_constraints) for s in string_list]
            for f in as_completed(futures):
                string, results = f.result()
                str_feature_map[string] = results
        logging.info(f"Computed features for {len(str_feature_map)} strings.")

        # select strings with matching features
        valid_strings = []
        for string, feature_values in str_feature_map.items():
            valid = True
            for k, v in feature_values.items():
                fc = feature_constraints[k]["value_range"]  # feature constraint
                if not (fc[0] <= v <= fc[1]):
                    valid = False
                    break
            if valid:
                valid_strings.append(string)

        # randomly select n_samples strings from the valid strings
        if len(valid_strings) < n_samples:
            logger.warning(
                f"Number of valid strings ({len(valid_strings)}) is less than the required n_samples ({n_samples})! Returning all valid strings..."
            )
            n_samples = len(valid_strings)

        valid_strings_features = []
        for i in self.rng.choice(len(valid_strings), n_samples, replace=False):
            # tuple of string and feature values (dict)
            valid_strings_features.append((list(valid_strings[i]), str_feature_map[valid_strings[i]]))

        return valid_strings_features

    def save(self, file_name=None, file_path=None):
        """
        Save the current ArtificialGrammar object to a file.

        Parameters
        ----------
        file_name : str, optional
            Name of the file to save. If None, the file name will be generated based on the label. Defaults to None.
        file_path : str, optional
            Path to the directory where the file should be saved. If None, the file will be saved in the current
            working directory. Defaults to None.

        """
        if file_name is None:
            file_name = f"AG_{self.label}.pkl"

        save_pickle(self, file_name, file_path)

    def plot_grammar(self, save=None, **kwargs):
        """
        Plot the grammar.

        Parameters
        ----------
        save : str, optional
            File name to save the plot. If None, the plot will be displayed. Defaults to None.

        Returns
        -------
        None
            The function does not return anything.
        """
        plot_grammar(self, save=save, **kwargs)


def _generate_string_worker(args):
    self_obj, gen_func, gen_kwargs, n_samples, child_ss = args
    self_obj.rng = np.random.default_rng(child_ss)
    if gen_func == "generate_string":
        return [self_obj.generate_string(**gen_kwargs) for _ in range(n_samples)]
    elif gen_func == "generate_nongrammatical_string":
        return [self_obj.generate_nongrammatical_string(**gen_kwargs) for _ in range(n_samples)]
    else:
        raise ValueError(f"Unknown generation function {gen_func}")
